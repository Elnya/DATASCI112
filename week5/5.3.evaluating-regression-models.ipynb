{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766096c4",
   "metadata": {},
   "source": [
    "# Evaluating Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c2760",
   "metadata": {},
   "source": [
    "## Training and Test Data\n",
    "\n",
    "The data for which we know the label $y$ is called the **training data.**\n",
    "\n",
    "The data for which we don't know $y$ (and want to predict it) is called the **test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4c8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/bordeaux.csv\", index_col=\"year\")\n",
    "df_train = df.loc[:1980].copy()\n",
    "df_test = df.loc[1981:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79072fe6",
   "metadata": {},
   "source": [
    "Let's seperate the inputs $X$ from the labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc6c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[[\"win\", \"summer\"]]\n",
    "y_train = df_train[\"price\"]\n",
    "\n",
    "X_test = df_test[[\"win\", \"summer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2f272",
   "metadata": {},
   "source": [
    "## $K$-Nearest Neighbors\n",
    "\n",
    "We've seen one machine learning model: $k$-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e016224b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.8, 54. , 52.2, 18.4, 35.6, 13.2, 37. , 51.4, 36.6, 36.6, 40.6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor(n_neighbors=5)\n",
    ")\n",
    "\n",
    "pipeline.fit(X=X_train, y=y_train)\n",
    "pipeline.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4526b",
   "metadata": {},
   "source": [
    "*Today*: How do we know if this model is any good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609af34",
   "metadata": {},
   "source": [
    "## Prediction Error\n",
    "\n",
    "If the true labels are $y_1, \\dots, y_n$ and our model predicts $\\hat{y}_1, \\dots, \\hat{y}_n,$ how do we measure how well our model did?\n",
    "\n",
    "- **mean squared error (MSE)**\n",
    "\n",
    "$$\n",
    "\\mathsf{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- **mean absolute error (MAE)**\n",
    "\n",
    "$$\n",
    "\\mathsf{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Calculating MSE or MAE requires data where true labels are known. Where can we find such data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f0d764",
   "metadata": {},
   "source": [
    "## Training Error\n",
    "\n",
    "On the training data, the true labels $y_1, \\dots, y_n$ are known.\n",
    "\n",
    "Let's calculate the **training error** of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cd8dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(207.24148148148146)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X=X_train, y=y_train)\n",
    "y_train_ = pipeline.predict(X=X_train)\n",
    "((y_train - y_train_) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6688d13",
   "metadata": {},
   "source": [
    "There's also a Scikit-Learn function for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102600dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.24148148148146"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true=y_train, y_pred=y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187d7b4",
   "metadata": {},
   "source": [
    "How do we intrepret this MSE of $207.24$? <br />\n",
    "Remember, we are predicting the price of wine. So the model is off by 208.24 square dollars on average.\n",
    "\n",
    "The square root is easier to interpret. The model is off by $\\sqrt{207.24} \\approx \\$14.40$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde224d",
   "metadata": {},
   "source": [
    "## The Problem with Training Error\n",
    "\n",
    "What's the training error of a $1$-nearest neighbor model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59707b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor(n_neighbors=1)\n",
    ")\n",
    "pipeline.fit(X=X_train, y=y_train)\n",
    "y_train_ = pipeline.predict(X=X_train)\n",
    "mean_squared_error(y_true=y_train, y_pred=y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a0a60",
   "metadata": {},
   "source": [
    "Why did this happen? <br />\n",
    "The 1-nearest neighbor to any observatoiin in the training data is the observation itself!\n",
    "\n",
    "A 1-nearest neighbor model will always be perfect on the training data. But is it necessarily the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd357e7",
   "metadata": {},
   "source": [
    "## Test Error\n",
    "\n",
    "We don't need to know how well our model does on *training data.*\n",
    "\n",
    "We want to know how well it will do on *test data.*\n",
    "\n",
    "In general, test error $>$ training error.\n",
    "\n",
    "Analogy: A professor posts a practice exam before an exam.\n",
    "- If the actual exam is the same as the practice exam, how many points will students miss? That's training error.\n",
    "- If the actual exam is different from the practice exam, how many points will students miss? That's test error.\n",
    "\n",
    "It's always easier to answer questions that you've seen before than questions you haven't seen.\n",
    "\n",
    "*Now:* How do we estimate the test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18446eef",
   "metadata": {},
   "source": [
    "## Validation Set\n",
    "\n",
    "The training data is the only data we have, where the true labels $y$ are known.\n",
    "\n",
    "So one way to estimate the test error is to not use all of the training data to fit the model, leaving the remaining data for estimating the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ca2e0",
   "metadata": {},
   "source": [
    "## Implementing the Validation Set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci112 (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
